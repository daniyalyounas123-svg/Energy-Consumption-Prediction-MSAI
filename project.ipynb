{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10387261,"sourceType":"datasetVersion","datasetId":6435081}],"dockerImageVersionId":31234,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-15T09:46:47.472409Z","iopub.execute_input":"2026-01-15T09:46:47.472750Z","iopub.status.idle":"2026-01-15T09:46:47.482809Z","shell.execute_reply.started":"2026-01-15T09:46:47.472720Z","shell.execute_reply":"2026-01-15T09:46:47.481859Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Basic Information","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Define the file paths\ntrain_path = '/kaggle/input/energy-consumption-dataset-linear-regression/train_energy_data.csv'\ntest_path = '/kaggle/input/energy-consumption-dataset-linear-regression/test_energy_data.csv'\n\n# Load the datasets\ntry:\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n\n    # 1. Basic Information (Columns, Data Types, Null Values)\n    print(\"--- Train Dataset Info ---\")\n    print(train_df.info())\n    \n    print(\"\\n--- Test Dataset Info ---\")\n    print(test_df.info())\n\n    # 2. First 5 Rows (Quick Preview)\n    print(\"\\n--- Train Dataset Preview (Head) ---\")\n    display(train_df.head())\n\n    # 3. Statistical Summary (Mean, Std, Min, Max, etc.)\n    print(\"\\n--- Train Dataset Statistics ---\")\n    display(train_df.describe())\n\n    # 4. Check for Missing Values\n    print(\"\\n--- Missing Values in Train Set ---\")\n    print(train_df.isnull().sum())\n\nexcept FileNotFoundError as e:\n    print(f\"Error: {e}. Please ensure the dataset is added to your Kaggle notebook.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T09:46:47.484299Z","iopub.execute_input":"2026-01-15T09:46:47.484572Z","iopub.status.idle":"2026-01-15T09:46:47.544979Z","shell.execute_reply.started":"2026-01-15T09:46:47.484548Z","shell.execute_reply":"2026-01-15T09:46:47.543999Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Statistical Distribution Analysis","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Plot distributions of numerical features and target\nnum_cols = ['Square Footage', 'Number of Occupants', 'Appliances Used', 'Average Temperature', 'Energy Consumption']\ntrain_df[num_cols].hist(bins=20, figsize=(15, 10))\nplt.suptitle(\"Feature Distributions\")\nplt.show()\n\n# Boxplots to check for outliers\nplt.figure(figsize=(12, 6))\nsns.boxplot(data=train_df[num_cols])\nplt.title(\"Outlier Detection\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T09:46:47.546575Z","iopub.execute_input":"2026-01-15T09:46:47.546865Z","iopub.status.idle":"2026-01-15T09:46:48.414186Z","shell.execute_reply.started":"2026-01-15T09:46:47.546839Z","shell.execute_reply":"2026-01-15T09:46:48.413201Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Categorical Impact (Feature vs. Target)","metadata":{}},{"cell_type":"code","source":"# Comparison of Energy Consumption across categories\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\nsns.boxplot(x='Building Type', y='Energy Consumption', data=train_df, ax=axes[0])\naxes[0].set_title('Energy Consumption by Building Type')\n\nsns.boxplot(x='Day of Week', y='Energy Consumption', data=train_df, ax=axes[1])\naxes[1].set_title('Energy Consumption by Day of Week')\nplt.xticks(rotation=45)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T09:46:48.416408Z","iopub.execute_input":"2026-01-15T09:46:48.416697Z","iopub.status.idle":"2026-01-15T09:46:48.858631Z","shell.execute_reply.started":"2026-01-15T09:46:48.416670Z","shell.execute_reply":"2026-01-15T09:46:48.857728Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Correlation & Multicollinearity","metadata":{}},{"cell_type":"code","source":"# Heatmap of correlations\nplt.figure(figsize=(10, 8))\nsns.heatmap(train_df[num_cols].corr(), annot=True, cmap='coolwarm', fmt='.2f')\nplt.title(\"Correlation Matrix\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T09:46:48.859799Z","iopub.execute_input":"2026-01-15T09:46:48.860338Z","iopub.status.idle":"2026-01-15T09:46:49.123316Z","shell.execute_reply.started":"2026-01-15T09:46:48.860299Z","shell.execute_reply":"2026-01-15T09:46:49.122344Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Linearity Check","metadata":{}},{"cell_type":"code","source":"# Scatter plots for linearity\nsns.pairplot(train_df, x_vars=['Square Footage', 'Average Temperature', 'Appliances Used'], \n             y_vars='Energy Consumption', height=5, aspect=0.8, kind='reg')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T09:46:49.124500Z","iopub.execute_input":"2026-01-15T09:46:49.124817Z","iopub.status.idle":"2026-01-15T09:46:50.022190Z","shell.execute_reply.started":"2026-01-15T09:46:49.124790Z","shell.execute_reply":"2026-01-15T09:46:50.021162Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Engineering & Preprocessing","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\n\n# Prepare Features and Target\nX = train_df.drop('Energy Consumption', axis=1)\ny = train_df['Energy Consumption']\n\n# Convert object columns to 'category' dtype for LightGBM\ncat_features = ['Building Type', 'Day of Week']\nfor col in cat_features:\n    X[col] = X[col].astype('category')\n\n# Split the data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T10:32:56.449190Z","iopub.execute_input":"2026-01-15T10:32:56.450048Z","iopub.status.idle":"2026-01-15T10:32:56.464580Z","shell.execute_reply.started":"2026-01-15T10:32:56.450001Z","shell.execute_reply":"2026-01-15T10:32:56.463548Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Initialization and Training","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb\n\n# 1. Initialize the Regressor\n# verbose=-1 suppresses the internal [Info] and [Warning] messages\nlgbm_model = lgb.LGBMRegressor(\n    n_estimators=1000,\n    learning_rate=0.05,\n    num_leaves=31,\n    random_state=42,\n    n_jobs=-1,\n    verbose=-1 \n)\n\n# 2. Fit the model\n# log_evaluation(period=1) ensures only the metrics per iteration are shown\n# early_stopping shows the final best iteration\nlgbm_model.fit(\n    X_train, y_train,\n    eval_set=[(X_val, y_val)],\n    eval_metric='rmse',\n    callbacks=[\n        lgb.early_stopping(stopping_rounds=50),\n        lgb.log_evaluation(period=10) # Shows metrics every 10 rounds to keep it clean\n    ]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T09:46:50.037522Z","iopub.execute_input":"2026-01-15T09:46:50.037774Z","iopub.status.idle":"2026-01-15T09:46:50.715544Z","shell.execute_reply.started":"2026-01-15T09:46:50.037750Z","shell.execute_reply":"2026-01-15T09:46:50.714745Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Evaluation","metadata":{}},{"cell_type":"code","source":"# Predictions\ny_pred = lgbm_model.predict(X_val)\n\n# Metrics\nrmse = np.sqrt(mean_squared_error(y_val, y_pred))\nr2 = r2_score(y_val, y_pred)\n\nprint(f\"Validation RMSE: {rmse:.4f}\")\nprint(f\"Validation R2 Score: {r2:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T09:46:50.716334Z","iopub.execute_input":"2026-01-15T09:46:50.716588Z","iopub.status.idle":"2026-01-15T09:46:50.737430Z","shell.execute_reply.started":"2026-01-15T09:46:50.716564Z","shell.execute_reply":"2026-01-15T09:46:50.736745Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Importance Visualization","metadata":{}},{"cell_type":"code","source":"# Plot Feature Importance\nplt.figure(figsize=(10, 6))\nlgb.plot_importance(lgbm_model, importance_type='split', max_num_features=10)\nplt.title(\"LightGBM Feature Importance\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T09:46:50.739633Z","iopub.execute_input":"2026-01-15T09:46:50.739897Z","iopub.status.idle":"2026-01-15T09:46:50.898263Z","shell.execute_reply.started":"2026-01-15T09:46:50.739869Z","shell.execute_reply":"2026-01-15T09:46:50.897362Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Final Prediction on Test Set","metadata":{}},{"cell_type":"code","source":"# Prepare Test Set\nX_test = test_df.drop('Energy Consumption', axis=1)\nfor col in cat_features:\n    X_test[col] = X_test[col].astype('category')\n\n# Predict\ntest_predictions = lgbm_model.predict(X_test)\n\n# Add to test_df for viewing\ntest_df['Predicted_Energy'] = test_predictions\nprint(test_df[['Building Type', 'Energy Consumption', 'Predicted_Energy']].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T09:46:50.899411Z","iopub.execute_input":"2026-01-15T09:46:50.899774Z","iopub.status.idle":"2026-01-15T09:46:50.922727Z","shell.execute_reply.started":"2026-01-15T09:46:50.899736Z","shell.execute_reply":"2026-01-15T09:46:50.921580Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Randomized Search","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nimport lightgbm as lgb\n\n# Define the parameter distribution\nparam_dist = {\n    'n_estimators': [500, 1000],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'num_leaves': [20, 31, 50, 70],\n    'max_depth': [-1, 10, 20],\n    'min_child_samples': [10, 20, 30],\n    'subsample': [0.8, 0.9, 1.0],\n    'colsample_bytree': [0.8, 0.9, 1.0]\n}\n\n# Initialize the base model\nlgbm = lgb.LGBMRegressor(random_state=42, verbose=-1)\n\n# Initialize RandomizedSearchCV\nrandom_search = RandomizedSearchCV(\n    estimator=lgbm,\n    param_distributions=param_dist,\n    n_iter=50,             # Number of parameter settings sampled\n    scoring='neg_mean_squared_error',\n    cv=3,                  # 3-fold cross-validation\n    verbose=1,\n    random_state=42,\n    n_jobs=-1\n)\n\n# Fit the search\nrandom_search.fit(X_train, y_train)\n\nprint(f\"Best Parameters (Random Search): {random_search.best_params_}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T09:46:50.923766Z","iopub.execute_input":"2026-01-15T09:46:50.925272Z","iopub.status.idle":"2026-01-15T09:47:59.630065Z","shell.execute_reply.started":"2026-01-15T09:46:50.925212Z","shell.execute_reply":"2026-01-15T09:47:59.629141Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Optuna","metadata":{}},{"cell_type":"code","source":"import optuna\nfrom sklearn.metrics import mean_squared_error\n\ndef objective(trial):\n    # Define the search space\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 500, 2000),\n        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n        'max_depth': trial.suggest_int('max_depth', 3, 12),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n        'verbose': -1\n    }\n\n    # Train model with current trial parameters\n    model = lgb.LGBMRegressor(**params)\n    model.fit(X_train, y_train)\n    \n    # Evaluate on validation set\n    preds = model.predict(X_val)\n    rmse = mean_squared_error(y_val, preds, squared=False)\n    return rmse\n\n# Create a study and optimize\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=50)\n\nprint(f\"Best Trial RMSE: {study.best_value}\")\nprint(f\"Best Parameters (Optuna): {study.best_params}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T09:47:59.631487Z","iopub.execute_input":"2026-01-15T09:47:59.632076Z","iopub.status.idle":"2026-01-15T09:48:00.523104Z","shell.execute_reply.started":"2026-01-15T09:47:59.632038Z","shell.execute_reply":"2026-01-15T09:48:00.521937Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Final Model & Reproducibility","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb\nimport pandas as pd\nimport numpy as np\nimport joblib # For saving the model\n\n# Set seed for reproducibility\nSEED = 42\n\n# Final Best Parameters from Optuna\nbest_params = {\n    'n_estimators': 1538,\n    'learning_rate': 0.03402359183605272,\n    'num_leaves': 103,\n    'max_depth': 5,\n    'min_child_samples': 85,\n    'lambda_l1': 0.001852547182792142,\n    'lambda_l2': 0.0001965599894239328,\n    'random_state': SEED,\n    'verbose': -1\n}\n\n# Preprocessing Function (consistent across train/test)\ndef preprocess_data(df):\n    temp_df = df.copy()\n    temp_df.columns = [c.replace(' ', '_') for c in temp_df.columns]\n    cat_cols = ['Building_Type', 'Day_of_Week']\n    for col in cat_cols:\n        temp_df[col] = temp_df[col].astype('category')\n    return temp_df\n\n# Prepare final data\nfinal_train = preprocess_data(train_df)\nX_final = final_train.drop('Energy_Consumption', axis=1)\ny_final = final_train['Energy_Consumption']\n\n# Train final model\nfinal_model = lgb.LGBMRegressor(**best_params)\nfinal_model.fit(X_final, y_final)\n\n# Save the model for demo/deployment\njoblib.dump(final_model, 'energy_lgbm_model.pkl')\nprint(\"Model finalized and saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T09:48:00.524454Z","iopub.status.idle":"2026-01-15T09:48:00.524992Z","shell.execute_reply.started":"2026-01-15T09:48:00.524729Z","shell.execute_reply":"2026-01-15T09:48:00.524759Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Advanced Visualizations for Presentation","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 1. Actual vs Predicted (Validation Set)\nval_preds = final_model.predict(X_val) # Use your previous X_val\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x=y_val, y=val_preds, alpha=0.5)\nplt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], '--r', lw=2)\nplt.title('Actual vs. Predicted Energy Consumption')\nplt.xlabel('Actual Values')\nplt.ylabel('Predictions')\nplt.show()\n\n# 2. Feature Importance (SHAP values are best for presentations)\n# If SHAP is not installed, use plot_importance\nplt.figure(figsize=(10, 8))\nlgb.plot_importance(final_model, importance_type='gain', precision=0)\nplt.title('Key Drivers of Energy Consumption')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T09:48:00.526115Z","iopub.status.idle":"2026-01-15T09:48:00.526404Z","shell.execute_reply.started":"2026-01-15T09:48:00.526266Z","shell.execute_reply":"2026-01-15T09:48:00.526285Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\ndef load_and_clean_data(filepath):\n    # 1. Ingestion\n    df = pd.read_csv(filepath)\n    \n    # 2. Cleaning: Convert to datetime and sort\n    # Replace 'Datetime' with your actual column name\n    df['Datetime'] = pd.to_datetime(df['Datetime'])\n    df = df.sort_values('Datetime')\n    df = df.set_index('Datetime')\n    \n    # Handle duplicates or missing values\n    df = df[~df.index.duplicated(keep='first')]\n    df = df.resample('H').mean().ffill() # Ensure hourly continuity\n    return df\n\ndef feature_engineering(df):\n    # 3. Basic Feature Extraction\n    df['hour'] = df.index.hour\n    df['day_of_week'] = df.index.dayofweek\n    df['month'] = df.index.month\n    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n    return df\n\ndef establish_baseline(df, target_col):\n    # 4. Baseline: Predict that tomorrow same hour = today same hour\n    # We shift by 24 hours to create a 'Persistence' baseline\n    df['baseline_pred'] = df[target_col].shift(24)\n    \n    # Drop rows where we don't have a baseline yet (first 24 hours)\n    valid_data = df.dropna()\n    \n    mae = mean_absolute_error(valid_data[target_col], valid_data['baseline_pred'])\n    rmse = np.sqrt(mean_squared_error(valid_data[target_col], valid_data['baseline_pred']))\n    \n    print(f\"--- Baseline Performance (Persistence Model) ---\")\n    print(f\"MAE: {mae:.2f}\")\n    print(f\"RMSE: {rmse:.2f}\")\n    return valid_data\n\n# Usage\n# df = load_and_clean_data('energy_consumption.csv')\n# df = feature_engineering(df)\n# df_final = establish_baseline(df, 'MW_Consumption')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T09:48:00.527360Z","iopub.status.idle":"2026-01-15T09:48:00.527647Z","shell.execute_reply.started":"2026-01-15T09:48:00.527500Z","shell.execute_reply":"2026-01-15T09:48:00.527517Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef prepare_energy_data(file_path, window_size=24):\n    # 1. Load Data\n    df = pd.read_csv(file_path, parse_dates=['Datetime'], index_col='Datetime')\n    df = df.sort_index()\n\n    # 2. Feature Engineering: Extract time-based features\n    df['hour'] = df.index.hour\n    df['day_of_week'] = df.index.dayofweek\n    df['month'] = df.index.month\n    \n    # 3. Scaling\n    scaler = MinMaxScaler()\n    scaled_data = scaler.fit_transform(df)\n\n    # 4. Create Windows (X = last 24hrs, y = next hr)\n    X, y = [], []\n    for i in range(window_size, len(scaled_data)):\n        X.append(scaled_data[i-window_size:i])\n        y.append(scaled_data[i, 0]) # Predicting the first column (Energy Consumption)\n\n    return np.array(X), np.array(y), scaler\n\n# Example Usage:\n# X_train, y_train, scaler = prepare_energy_data('pjm_energy_data.csv')\n# print(f\"Input Shape (Samples, Time Steps, Features): {X_train.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T09:49:51.488481Z","iopub.execute_input":"2026-01-15T09:49:51.489441Z","iopub.status.idle":"2026-01-15T09:49:51.496788Z","shell.execute_reply.started":"2026-01-15T09:49:51.489405Z","shell.execute_reply":"2026-01-15T09:49:51.495826Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, mean_squared_error\n\n# 1. Load the dataset\n# Adjust the path if your file name is different (e.g., 'train_energy_data.csv')\nfile_path = '/kaggle/input/energy-consumption-dataset-linear-regression/Energy_consumption.csv'\ntry:\n    df = pd.read_csv(file_path)\nexcept FileNotFoundError:\n    print(\"File not found. Please check the dataset path.\")\n    # For demonstration, creating dummy data matching the dataset structure\n    data = {\n        'Building Type': np.random.choice(['Residential', 'Commercial', 'Industrial'], 1000),\n        'Square Footage': np.random.randint(1000, 50000, 1000),\n        'Number of Occupants': np.random.randint(1, 100, 1000),\n        'Appliances Used': np.random.randint(1, 50, 1000),\n        'Average Temperature': np.random.uniform(10, 35, 1000),\n        'Day of Week': np.random.choice(['Weekday', 'Weekend'], 1000),\n        'Energy Consumption': np.random.uniform(1000, 7000, 1000)\n    }\n    df = pd.DataFrame(data)\n\n# 2. Preprocessing\n# Encoding categorical variables\nle = LabelEncoder()\ndf['Building Type'] = le.fit_transform(df['Building Type'])\ndf['Day of Week'] = le.fit_transform(df['Day of Week'])\n\nX = df.drop('Energy Consumption', axis=1)\ny = df['Energy Consumption']\n\n# Splitting data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Scaling features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# 3. Model Training & Stability (Loss vs Iterations)\n# Using SGDRegressor to capture the loss curve over iterations\nmodel = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True, random_state=42)\ntrain_loss = []\ntest_loss = []\n\nepochs = 100\nfor epoch in range(epochs):\n    model.partial_fit(X_train_scaled, y_train)\n    \n    y_train_pred = model.predict(X_train_scaled)\n    y_test_pred = model.predict(X_test_scaled)\n    \n    train_loss.append(mean_squared_error(y_train, y_train_pred))\n    test_loss.append(mean_squared_error(y_test, y_test_pred))\n\n# Plotting Training Stability\nplt.figure(figsize=(10, 5))\nplt.plot(train_loss, label='Training Loss (MSE)')\nplt.plot(test_loss, label='Validation Loss (MSE)')\nplt.title('Training Stability (Loss vs Epochs)')\nplt.xlabel('Epochs')\nplt.ylabel('Mean Squared Error')\nplt.legend()\nplt.grid(True)\nplt.savefig('training_stability.png')\nplt.show()\n\n# 4. Confusion Matrix (Error Analysis)\n# Since regression is continuous, we bin values into categories: Low, Medium, High\ndef bin_energy(values):\n    bins = np.quantile(y, [0, 0.33, 0.66, 1.0])\n    return np.digitize(values, bins[1:-1])\n\ny_test_binned = bin_energy(y_test)\ny_pred_binned = bin_energy(y_test_pred)\n\nlabels = ['Low', 'Medium', 'High']\ncm = confusion_matrix(y_test_binned, y_pred_binned)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=labels, yticklabels=labels)\nplt.title('Confusion Matrix (Error Analysis via Binning)')\nplt.xlabel('Predicted Category')\nplt.ylabel('Actual Category')\nplt.savefig('confusion_matrix.png')\nplt.show()\n\nprint(\"Analysis Complete. Images saved as 'training_stability.png' and 'confusion_matrix.png'.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T10:29:17.377080Z","iopub.execute_input":"2026-01-15T10:29:17.377422Z","iopub.status.idle":"2026-01-15T10:29:17.435008Z","shell.execute_reply.started":"2026-01-15T10:29:17.377392Z","shell.execute_reply":"2026-01-15T10:29:17.433956Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.metrics import r2_score, confusion_matrix, ConfusionMatrixDisplay\n\n# 1. Load the dataset\ntrain_path = '/kaggle/input/energy-consumption-dataset-linear-regression/train_energy_data.csv'\ntest_path = '/kaggle/input/energy-consumption-dataset-linear-regression/test_energy_data.csv'\n\ntrain_df = pd.read_csv(train_path)\ntest_df = pd.read_csv(test_path)\n\n# 2. Preprocessing\n# Target variable: 'Energy Consumption'\nX_train = train_df.drop('Energy Consumption', axis=1)\ny_train = train_df['Energy Consumption']\nX_test = test_df.drop('Energy Consumption', axis=1)\ny_test = test_df['Energy Consumption']\n\ncategorical_cols = ['Building Type', 'Day of Week']\nnumerical_cols = ['Square Footage', 'Number of Occupants', 'Appliances Used', 'Average Temperature']\n\n# Create a preprocessor to handle scaling and encoding\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numerical_cols),\n        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n    ])\n\nX_train_proc = preprocessor.fit_transform(X_train)\nX_test_proc = preprocessor.transform(X_test)\n\n# 3. Training Stability (Accuracy vs Loss)\n# Using SGDRegressor to record metrics per \"epoch\"\nmodel = SGDRegressor(max_iter=1, tol=None, warm_start=True, learning_rate='constant', eta0=0.01)\nepochs = 100\nloss_history = []\naccuracy_history = []\n\nfor epoch in range(epochs):\n    model.partial_fit(X_train_proc, y_train)\n    y_pred_train = model.predict(X_train_proc)\n    \n    # Track Mean Squared Error (Loss) and R^2 Score (Accuracy Proxy)\n    mse = np.mean((y_train - y_pred_train) ** 2)\n    r2 = r2_score(y_train, y_pred_train)\n    \n    loss_history.append(mse)\n    accuracy_history.append(max(0, r2)) # Ensure R2 doesn't go below 0 for plotting\n\n# Plotting Training Stability\nfig, ax1 = plt.subplots(figsize=(10, 6))\n\nax1.set_xlabel('Epochs')\nax1.set_ylabel('Loss (MSE)', color='tab:red')\nax1.plot(range(epochs), loss_history, color='tab:red', label='Training Loss')\nax1.tick_params(axis='y', labelcolor='tab:red')\n\nax2 = ax1.twinx()\nax2.set_ylabel('R^2 Score (Accuracy)', color='tab:blue')\nax2.plot(range(epochs), accuracy_history, color='tab:blue', label='R^2 Score')\nax2.tick_params(axis='y', labelcolor='tab:blue')\n\nplt.title('Training Stability: Accuracy ($R^2$) vs Loss (MSE)')\nfig.tight_layout()\nplt.show()\n\n# 4. Confusion Matrix (Error Analysis via Binning)\ny_pred_test = model.predict(X_test_proc)\n\n# Convert continuous values into 3 categories: Low, Medium, High\ndef bin_energy(values, bins):\n    return np.digitize(values, bins)\n\n# Define bins based on the distribution of training data\nbins = np.percentile(y_train, [33, 66])\ncat_labels = ['Low', 'Medium', 'High']\n\ny_test_binned = bin_energy(y_test, bins)\ny_pred_binned = bin_energy(y_pred_test, bins)\n\ncm = confusion_matrix(y_test_binned, y_pred_binned)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=cat_labels, yticklabels=cat_labels)\nplt.title('Error Analysis: Confusion Matrix (Binned Consumption)')\nplt.xlabel('Predicted Consumption Category')\nplt.ylabel('Actual Consumption Category')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T10:43:38.588357Z","iopub.execute_input":"2026-01-15T10:43:38.588685Z","iopub.status.idle":"2026-01-15T10:43:39.235786Z","shell.execute_reply.started":"2026-01-15T10:43:38.588659Z","shell.execute_reply":"2026-01-15T10:43:39.234833Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n# 1. Load Data\ntrain_df = pd.read_csv('/kaggle/input/energy-consumption-dataset-linear-regression/train_energy_data.csv')\ntest_df = pd.read_csv('/kaggle/input/energy-consumption-dataset-linear-regression/test_energy_data.csv')\n\n# 2. Simple Preprocessing (Handling only numeric for brevity)\nX_train = train_df.select_dtypes(include=[np.number]).drop(columns=['Energy Consumption'])\ny_train = train_df['Energy Consumption']\nX_test = test_df.select_dtypes(include=[np.number]).drop(columns=['Energy Consumption'])\ny_test = test_df['Energy Consumption']\n\n# 3. Train Linear Regression Model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\n# 4. DISCRETIZATION (The \"Error Analysis\" Trick)\n# We define 3 categories: Low, Medium, High based on the distribution of energy\ndef categorize_energy(value, bins):\n    if value <= bins[0]: return 'Low'\n    if value <= bins[1]: return 'Medium'\n    return 'High'\n\n# Use percentiles to create fair bins (33rd and 66th percentile)\nbins = np.percentile(y_train, [33, 66])\n\ny_test_cat = [categorize_energy(v, bins) for v in y_test]\ny_pred_cat = [categorize_energy(v, bins) for v in y_pred]\nlabels = ['Low', 'Medium', 'High']\n\n# 5. Generate and Plot Confusion Matrix\ncm = confusion_matrix(y_test_cat, y_pred_cat, labels=labels)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Reds', \n            xticklabels=labels, yticklabels=labels)\n\nplt.xlabel('Predicted Energy Usage')\nplt.ylabel('Actual Energy Usage')\nplt.title('Error Analysis: Confusion Matrix (Binned Regression Results)')\nplt.show()\n\n# 6. Print Classification Report for deeper insights\nprint(classification_report(y_test_cat, y_pred_cat, target_names=labels))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T10:40:04.379619Z","iopub.execute_input":"2026-01-15T10:40:04.379988Z","iopub.status.idle":"2026-01-15T10:40:04.602575Z","shell.execute_reply.started":"2026-01-15T10:40:04.379957Z","shell.execute_reply":"2026-01-15T10:40:04.601499Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, r2_score\n\n# 1. Load the dataset\n# Assuming paths: /kaggle/input/energy-consumption-dataset-linear-regression/train_energy_data.csv\ntry:\n    train_df = pd.read_csv('/kaggle/input/energy-consumption-dataset-linear-regression/train_energy_data.csv')\n    test_df = pd.read_csv('/kaggle/input/energy-consumption-dataset-linear-regression/test_energy_data.csv')\nexcept FileNotFoundError:\n    print(\"Files not found. Please ensure the paths are correct.\")\n    # Exit or use dummy data for demonstration\n    exit()\n\n# 2. Preprocessing\n# Target: 'Energy Consumption'\n# Features based on dataset inspection: Building Type, Square Footage, Number of Occupants, \n# Appliances Used, Average Temperature, Day of Week\nX = train_df.drop('Energy Consumption', axis=1)\ny = train_df['Energy Consumption']\n\ncategorical_cols = ['Building Type', 'Day of Week']\nnumerical_cols = ['Square Footage', 'Number of Occupants', 'Appliances Used', 'Average Temperature']\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numerical_cols),\n        ('cat', OneHotEncoder(), categorical_cols)\n    ])\n\nX_processed = preprocessor.fit_transform(X)\nX_test_processed = preprocessor.transform(test_df.drop('Energy Consumption', axis=1))\ny_test = test_df['Energy Consumption']\n\n# 3. Training Stability (Loss vs. Epochs)\nmodel = SGDRegressor(max_iter=1, tol=None, warm_start=True, learning_rate='constant', eta0=0.01)\nepochs = 100\nlosses = []\naccuracies = [] # Using R^2 as proxy\n\nfor epoch in range(epochs):\n    model.partial_fit(X_processed, y)\n    y_pred_train = model.predict(X_processed)\n    \n    # Calculate Loss (MSE)\n    mse = np.mean((y - y_pred_train) ** 2)\n    losses.append(mse)\n    \n    # Calculate R^2 (Accuracy proxy)\n    r2 = r2_score(y, y_pred_train)\n    accuracies.append(max(0, r2)) # Clamp to 0 for visualization\n\n# Plot Training Stability\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(range(epochs), losses, color='red', label='Training Loss (MSE)')\nplt.title('Training Stability: Loss over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(range(epochs), accuracies, color='blue', label='R^2 Score (\"Accuracy\")')\nplt.title('Training Stability: R^2 Score over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('R^2 Score')\nplt.legend()\nplt.tight_layout()\nplt.savefig('stability_plots.png')\n\n# 4. Confusion Matrix (Error Analysis via Binning)\ny_pred_test = model.predict(X_test_processed)\n\n# Create bins (Low, Medium, High consumption)\nbins = np.percentile(y, [33, 66])\ndef categorize(val):\n    if val <= bins[0]: return 'Low'\n    if val <= bins[1]: return 'Medium'\n    return 'High'\n\ny_test_cat = [categorize(v) for v in y_test]\ny_pred_cat = [categorize(v) for v in y_pred_test]\nlabels = ['Low', 'Medium', 'High']\n\ncm = confusion_matrix(y_test_cat, y_pred_cat, labels=labels)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\nplt.title('Error Analysis: Confusion Matrix (Binned Energy Consumption)')\nplt.xlabel('Predicted Category')\nplt.ylabel('Actual Category')\nplt.savefig('confusion_matrix.png')\n\nprint(\"Plots saved: stability_plots.png and confusion_matrix.png\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T10:44:13.747452Z","iopub.execute_input":"2026-01-15T10:44:13.747823Z","iopub.status.idle":"2026-01-15T10:44:14.818803Z","shell.execute_reply.started":"2026-01-15T10:44:13.747793Z","shell.execute_reply":"2026-01-15T10:44:14.817834Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}